{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart - Inference\n",
    "\n",
    "This notebook covers a simple client usage, including the following points:\n",
    "- List available models.\n",
    "- Use the SambaNova inference adaptor to interact with cloud-based LLM chat models.\n",
    "- Implement a chat loop conversation using the SambaNova inference adaptor.\n",
    "\n",
    "Run inference via chat completions with the llama-stack Python SDK.\n",
    "\n",
    "Please refer to the [llama-stack quickstart documentation](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from llama_stack_client import LlamaStackClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HTTP client\n",
    "LLAMA_STACK_PORT = 8321\n",
    "client = LlamaStackClient(base_url=f\"http://localhost:{LLAMA_STACK_PORT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Available models: ---\n",
      "- sambanova/Meta-Llama-3.1-8B-Instruct\n",
      "- meta-llama/Llama-3.1-8B-Instruct\n",
      "- sambanova/Meta-Llama-3.1-405B-Instruct\n",
      "- meta-llama/Llama-3.1-405B-Instruct-FP8\n",
      "- sambanova/Meta-Llama-3.2-1B-Instruct\n",
      "- meta-llama/Llama-3.2-1B-Instruct\n",
      "- sambanova/Meta-Llama-3.2-3B-Instruct\n",
      "- meta-llama/Llama-3.2-3B-Instruct\n",
      "- sambanova/Meta-Llama-3.3-70B-Instruct\n",
      "- meta-llama/Llama-3.3-70B-Instruct\n",
      "- sambanova/Llama-3.2-11B-Vision-Instruct\n",
      "- meta-llama/Llama-3.2-11B-Vision-Instruct\n",
      "- sambanova/Llama-3.2-90B-Vision-Instruct\n",
      "- meta-llama/Llama-3.2-90B-Vision-Instruct\n",
      "- sambanova/Llama-4-Scout-17B-16E-Instruct\n",
      "- meta-llama/Llama-4-Scout-17B-16E-Instruct\n",
      "- sambanova/Llama-4-Maverick-17B-128E-Instruct\n",
      "- meta-llama/Llama-4-Maverick-17B-128E-Instruct\n",
      "- sambanova/Meta-Llama-Guard-3-8B\n",
      "- meta-llama/Llama-Guard-3-8B\n",
      "- all-MiniLM-L6-v2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"- {m.identifier}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an inference model from the previous list\n",
    "model = \"sambanova/Meta-Llama-3.3-70B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Chat Completion Request\n",
    "Use the `chat_completion` function to define the conversation context. Each message you include should have a specific role and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With gentle eyes and a soft, fuzzy face, the llama roams the Andes with a peaceful, gentle pace. Its long neck bends as it grazes with glee, a symbol of serenity in a world wild and free.\n"
     ]
    }
   ],
   "source": [
    "response = client.inference.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a friendly assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a two-sentence poem about llama.\"},\n",
    "    ],\n",
    "    model_id=model,\n",
    ")\n",
    "\n",
    "print(response.completion_message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation Loop\n",
    "To create a continuous conversation loop, where users can input multiple messages in a session, use the following structure. This example runs an asynchronous loop, ending when the user types 'exit,' 'quit,' or 'bye.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m> User: Hi, Tell me a joke\u001b[0m\n",
      "\u001b[36m> Response: Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you laugh! Do you want to hear another one?\u001b[0m\n",
      "\u001b[32m> User: what is the capital of Austria\u001b[0m\n",
      "\u001b[36m> Response: The capital of Austria is Vienna (German: Wien).\u001b[0m\n",
      "\u001b[32m> User: quit\u001b[0m\n",
      "\u001b[33mEnding conversation. Goodbye!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from termcolor import cprint\n",
    "\n",
    "async def chat_loop():\n",
    "    while True:\n",
    "        user_input = input(\"User> \")\n",
    "        cprint(f\"> User: {user_input}\", \"green\")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            cprint(\"Ending conversation. Goodbye!\", \"yellow\")\n",
    "            break\n",
    "\n",
    "        message = {\"role\": \"user\", \"content\": user_input}\n",
    "        response = client.inference.chat_completion(messages=[message], model_id=model)\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "\n",
    "\n",
    "# Run the chat loop in a Jupyter Notebook cell using await\n",
    "await chat_loop()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(chat_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversation History\n",
    "Maintaining a conversation history allows the model to retain context from previous interactions. Use a list to accumulate messages, enabling continuity throughout the chat session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m> User: Hi, I want to learn spanish\u001b[0m\n",
      "\u001b[36m> Response: ¡Hola! Learning Spanish can be a rewarding and enriching experience. With over 460 million native speakers, Spanish is the second most widely spoken language in the world, and it's an official language in 20 countries.\n",
      "\n",
      "To get started, let's break down the basics:\n",
      "\n",
      "1. **Alphabet**: Spanish uses the same alphabet as English, with a few additional letters like ñ, ü, and ll.\n",
      "2. **Pronunciation**: Spanish pronunciation is generally phonetic, meaning that words are pronounced as they're written. Pay attention to accents and diacritical marks, as they can change the pronunciation of words.\n",
      "3. **Grammar**: Spanish grammar is relatively similar to English grammar, with a few key differences. For example, Spanish has two forms of the verb \"to be\" (ser and estar), and it uses verb conjugations to indicate tense and mood.\n",
      "\n",
      "Here are some beginner-friendly resources to help you learn Spanish:\n",
      "\n",
      "* **Duolingo**: A popular language-learning app that offers interactive lessons and exercises.\n",
      "* **SpanishDict**: A comprehensive online dictionary and grammar guide.\n",
      "* **BBC Languages**: A website with video and audio lessons, as well as interactive exercises and quizzes.\n",
      "* **Spanish language exchange websites**: Sites like italki and Conversation Exchange can connect you with native Spanish speakers for language exchange and practice.\n",
      "\n",
      "What's your current level of Spanish proficiency? Are you a complete beginner, or do you have some experience with the language?\n",
      "\n",
      "Also, what are your goals for learning Spanish? Are you interested in:\n",
      "\n",
      "* Traveling to Spanish-speaking countries?\n",
      "* Improving your career prospects?\n",
      "* Connecting with Spanish-speaking family or friends?\n",
      "* Enhancing your cognitive abilities?\n",
      "\n",
      "Let me know, and I'll be happy to help you get started on your Spanish learning journey!\u001b[0m\n",
      "\u001b[32m> User: traveling to Colombia\u001b[0m\n",
      "\u001b[36m> Response: ¡Excelente elección! Colombia is a beautiful country with a rich culture, stunning landscapes, and friendly people. Learning Spanish will definitely enhance your travel experience and allow you to connect with the locals.\n",
      "\n",
      "As a traveler to Colombia, you'll want to focus on learning practical Spanish phrases and vocabulary that will help you navigate everyday situations. Here are some essential topics to get you started:\n",
      "\n",
      "1. **Greetings and introductions**: Learn basic phrases like \"hola\" (hello), \"gracias\" (thank you), \"¿cómo estás?\" (how are you?), and \"me llamo\" (my name is).\n",
      "2. **Food and drink**: Familiarize yourself with common food and drink vocabulary, such as \"comida\" (food), \"agua\" (water), \"café\" (coffee), and \"cerveza\" (beer).\n",
      "3. **Directions and transportation**: Learn phrases like \"¿dónde está...?\" (where is...?), \"¿cómo se llama?\" (what's it called?), and \"¿cuánto cuesta?\" (how much does it cost?).\n",
      "4. **Accommodation and shopping**: Learn vocabulary related to hotels, hostels, and shopping, such as \"hotel\" (hotel), \"habitación\" (room), and \"tienda\" (store).\n",
      "5. **Basic phrases for emergencies**: Learn phrases like \"¿dónde está el baño?\" (where is the bathroom?), \"¿dónde está la estación de policía?\" (where is the police station?), and \"necesito ayuda\" (I need help).\n",
      "\n",
      "Some useful Colombian slang and expressions to keep in mind:\n",
      "\n",
      "* \"¿Qué más?\" (what's up?) - a casual greeting\n",
      "* \"¡Hagámoslo!\" (let's do it!) - an expression of enthusiasm\n",
      "* \"¡Esto es una pasada!\" (this is awesome!) - an expression of excitement\n",
      "* \"¡Vale más!\" (you're welcome!) - a response to \"gracias\"\n",
      "\n",
      "To get you started, here are some beginner-friendly resources:\n",
      "\n",
      "* **Colombian Spanish podcasts**: Listen to podcasts like \"Coffee Break Spanish\" or \"Spanish Obsessed\" to get a feel for the Colombian accent and vocabulary.\n",
      "* **Travel guides and phrasebooks**: Check out travel guides like Lonely Planet or phrasebooks like \"Colombian Spanish Phrasebook\" to learn essential phrases and vocabulary.\n",
      "* **Language exchange websites**: Use websites like italki or Conversation Exchange to practice your Spanish with native speakers from Colombia.\n",
      "\n",
      "What's your current level of Spanish proficiency? Are you a complete beginner, or do you have some experience with the language?\n",
      "\n",
      "Also, when are you planning to travel to Colombia? Having a specific goal in mind will help you stay motivated and focused in your language learning journey!\u001b[0m\n",
      "\u001b[32m> User: bye\u001b[0m\n",
      "\u001b[33mEnding conversation. Goodbye!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "async def chat_loop():\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        user_input = input(\"User> \")\n",
    "        cprint(f\"> User: {user_input}\", \"green\")\n",
    "        if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
    "            cprint(\"Ending conversation. Goodbye!\", \"yellow\")\n",
    "            break\n",
    "\n",
    "        user_message = {\"role\": \"user\", \"content\": user_input}\n",
    "        conversation_history.append(user_message)\n",
    "\n",
    "        response = client.inference.chat_completion(\n",
    "            messages=conversation_history,\n",
    "            model_id=model,\n",
    "        )\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "\n",
    "        # Append the assistant message with all required fields\n",
    "        assistant_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": response.completion_message.content,\n",
    "            # Add any additional required fields here if necessary\n",
    "        }\n",
    "        conversation_history.append(assistant_message)\n",
    "\n",
    "\n",
    "# Use `await` in the Jupyter Notebook cell to call the function\n",
    "await chat_loop()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(chat_loop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Responses\n",
    "Llama Stack offers a stream parameter in the chat_completion function, which allows partial responses to be returned progressively as they are generated. This can enhance user experience by providing immediate feedback without waiting for the entire response to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mUser> Please write me a 3 sentence poem about llamas.\u001b[0m\n",
      "\u001b[36mAssistant> \u001b[0m\u001b[33mHere\u001b[0m\u001b[33m is\u001b[0m\u001b[33m a 3 sentence poem about llamas:\n",
      "\u001b[0m\u001b[33mL\u001b[0m\u001b[33mlamas roam the Andean highlands with\u001b[0m\u001b[33m gentle ease,\u001b[0m\u001b[33m their soft fur a warm\u001b[0m\u001b[33m and fuzzy breeze\u001b[0m\u001b[33m. With ears\u001b[0m\u001b[33m so tall\u001b[0m\u001b[33m and eyes so bright, they watch the world with\u001b[0m\u001b[33m quiet\u001b[0m\u001b[33m delight. In their tranquil\u001b[0m\u001b[33m presence, all worries cease\u001b[0m\u001b[33m,\u001b[0m\u001b[33m and peace desc\u001b[0m\u001b[33mends like\u001b[0m\u001b[33m a soft\u001b[0m\u001b[33m, llama\u001b[0m\u001b[33m-filled\u001b[0m\u001b[33m release\u001b[0m\u001b[33m.\u001b[0m\u001b[97m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from llama_stack_client.lib.inference.event_logger import EventLogger\n",
    "\n",
    "async def run_main(stream: bool = True):\n",
    "    message = {\"role\": \"user\", \"content\": \"Please write me a 3 sentence poem about llamas.\"}\n",
    "    cprint(f'User> {message[\"content\"]}', \"green\")\n",
    "\n",
    "    response = client.inference.chat_completion(\n",
    "        messages=[message],\n",
    "        model_id=model,\n",
    "        stream=stream,\n",
    "    )\n",
    "\n",
    "    if not stream:\n",
    "        cprint(f\"> Response: {response.completion_message.content}\", \"cyan\")\n",
    "    else:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "\n",
    "\n",
    "# In a Jupyter Notebook cell, use `await` to call the function\n",
    "await run_main()\n",
    "# To run it in a python file, use this line instead\n",
    "# asyncio.run(run_main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamastackenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
